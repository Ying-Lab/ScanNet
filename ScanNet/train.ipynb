{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import datetime\n",
    "import argparse,copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import  DataLoader\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.manifold import TSNE\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('./ScanNet')\n",
    "from scannet import *\n",
    "from datasets import GCNDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='description of')\n",
    "parser.add_argument('--dataset', default='Muraro', type=str)\n",
    "parser.add_argument('--cross_protocol', default=False, type=bool)\n",
    "parser.add_argument('--lr', default=0.01, type=float, help='Initial learning rate')\n",
    "parser.add_argument('--weight_decay', default=5e-4, type=float, help='Weight decay (L2 loss on parameters)')\n",
    "parser.add_argument('--type_fusion', default='att', type=str, help='fusion method')\n",
    "parser.add_argument('--type_att_size', default=32, type=int, help='attention parameter dimension')\n",
    "parser.add_argument('--cuda', default=True, type=bool, help='cpu or gpu') \n",
    "parser.add_argument('--epochs', default=60, type=int, help='Number of epoch')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='Number of batch size')\n",
    "parser.add_argument('--in_dim', default=1, type=int, help='dim of input')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda and torch.cuda.is_available():\n",
    "        device=torch.device('cuda')\n",
    "else:\n",
    "        device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadbench(dataset):\n",
    "    data_dir='./data/pbmcbench/data_pbmcbench.csv'\n",
    "    data=pd.read_csv(data_dir,index_col=0,header=0)\n",
    "    train_df=data[data['protocol'].isin(dataset.split('+'))]\n",
    "    test_df=data[~data['protocol'].isin(dataset.split('+'))]\n",
    "    adj_tf_gene=pd.read_csv('./data/pbmcbench/cpmadj_tf_gene.csv',index_col=0,header=0)\n",
    "    tf_num=adj_tf_gene.shape[0]\n",
    "    gene_num=adj_tf_gene.shape[1]\n",
    "    adj_tf_gene=torch.tensor(adj_tf_gene.values,dtype=torch.float32)\n",
    "    test_tf=test_df.iloc[:,:tf_num].values\n",
    "    test_gene=test_df.iloc[:,tf_num:tf_num+gene_num].values\n",
    "    label_test=torch.tensor(test_df['cell_type_label'].values,dtype=torch.int64)\n",
    "    ft_dict_test=[]\n",
    "    for i in range(test_tf.shape[0]):\n",
    "        ft_dict={'tf':torch.tensor(test_tf[i].reshape(-1,1),dtype=torch.float32),'gene':torch.tensor(test_gene[i].reshape(-1,1),dtype=torch.float32)}\n",
    "        ft_dict_test.append(ft_dict)\n",
    "    train_tf=train_df.iloc[:,:tf_num].values\n",
    "    train_gene=train_df.iloc[:,tf_num:tf_num+gene_num].values\n",
    "    label_sup=torch.tensor(train_df['cell_type_label'].values,dtype=torch.int64)\n",
    "    ft_dict_sup=[]\n",
    "    for i in range(train_tf.shape[0]):\n",
    "        ft_dict={'tf':torch.tensor(train_tf[i].reshape(-1,1),dtype=torch.float32),'gene':torch.tensor(train_gene[i].reshape(-1,1),dtype=torch.float32)}\n",
    "        ft_dict_sup.append(ft_dict)\n",
    "    ft_dict_train,ft_dict_valid,label_train,label_valid = train_test_split(ft_dict_sup,label_sup,test_size=0.2,random_state=42,stratify=label_sup) \n",
    "    return ft_dict_train,ft_dict_valid,ft_dict_test,label_train,label_valid,label_test,adj_tf_gene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25076/2425366793.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ft_dict_list=torch.load(load_path+'logft_dict_list.pt')\n",
      "/tmp/ipykernel_25076/2425366793.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  label=torch.load(load_path+'label.pt')\n",
      "/tmp/ipykernel_25076/2425366793.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adj_tf_gene=torch.load(load_path+'cpmadj_tf_gene.pt')\n"
     ]
    }
   ],
   "source": [
    "if not args.cross_protocol:\n",
    "    load_path='./data/{}/'.format(args.dataset)\n",
    "    ft_dict_list=torch.load(load_path+'logft_dict_list.pt')\n",
    "    label=torch.load(load_path+'label.pt')\n",
    "    adj_tf_gene=torch.load(load_path+'cpmadj_tf_gene.pt')\n",
    "    ft_dict_sup,ft_dict_test,label_sup,label_test = train_test_split(ft_dict_list,label,test_size=0.3,random_state=42,stratify=label)\n",
    "    ft_dict_train,ft_dict_valid,label_train,label_valid = train_test_split(ft_dict_sup,label_sup,test_size=0.2,random_state=42,stratify=label_sup) \n",
    "else:\n",
    "    ft_dict_train,ft_dict_valid,ft_dict_test,label_train,label_valid,label_test,adj_tf_gene=loadbench(args.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得初始化的adj_dict\n",
    "adj_dict={'tf':None,'gene':None}\n",
    "adj_gene_tf=adj_tf_gene.T\n",
    "\n",
    "degree_tf=torch.abs(adj_tf_gene).sum(dim=1)\n",
    "degree_tf_inv=torch.pow(degree_tf, -0.5)\n",
    "degree_gene=torch.abs(adj_tf_gene).sum(dim=0)\n",
    "degree_gene_inv=torch.pow(degree_gene, -0.5)\n",
    "D_tf_inv=torch.diag_embed(degree_tf_inv)\n",
    "D_gene_inv=torch.diag_embed(degree_gene_inv)\n",
    "adj_dict['tf']=torch.matmul(torch.matmul(D_tf_inv,adj_tf_gene),D_gene_inv)\n",
    "\n",
    "adj_dict['gene']=adj_dict['tf'].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements = torch.unique(label_test,return_inverse=False)\n",
    "args.class_num=len(unique_elements)\n",
    "# dataloader\n",
    "dataset={'train':None,'valid':None,'test':None}\n",
    "dataset['train']=GCNDataset(args,ft_dict_train,label_train)\n",
    "dataset['valid']=GCNDataset(args,ft_dict_valid,label_valid)\n",
    "\n",
    "dataloader={'train':None,'valid':None,'test':None}\n",
    "dataloader['train']=DataLoader(dataset['train'],batch_size=args.batch_size,shuffle=True,pin_memory=True)\n",
    "dataloader['valid']=DataLoader(dataset['valid'],batch_size=args.batch_size,shuffle=True,pin_memory=True)\n",
    "args.train_size=label_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "net_schema = {'tf':['gene'],'gene':['tf']}\n",
    "type_nodes={'tf':len(ft_dict_test[0]['tf']),'gene':len(ft_dict_test[0]['gene'])}\n",
    "all_nodes=len(ft_dict_test[0]['tf'])+len(ft_dict_test[0]['gene'])\n",
    "tf_nodes=len(ft_dict_test[0]['tf'])\n",
    "layer_shape=[args.in_dim,8,16,32,args.class_num]\n",
    "model = ScanNet(\n",
    "            net_schema=net_schema,\n",
    "            layer_shape=layer_shape,\n",
    "            all_nodes=all_nodes,\n",
    "            tf_nodes=tf_nodes,\n",
    "            type_fusion=args.type_fusion,\n",
    "            type_att_size=args.type_att_size,\n",
    "            )\n",
    "model=model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), momentum=0.9, lr= args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "decay = 0.95\n",
    "decay_steps = args.train_size\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    lr = lr * pow( decay , float(global_step// decay_steps) ) # decay by one epoch\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "best_val_f1 = float(0) \n",
    "best_model_weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train() \n",
    "    total_loss = 0    \n",
    "    all_labels=[]\n",
    "    all_predictions=[]\n",
    "    for k in adj_dict.keys():\n",
    "        adj_dict[k]=adj_dict[k].to(device)\n",
    "    global global_step,best_val_f1,best_model_weights\n",
    "    cur_lr = adjust_learning_rate(optimizer, args.lr)\n",
    "    \n",
    "    for i,imbalanced_batch in enumerate(dataloader['train']):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        im_ft_dict,im_label=imbalanced_batch\n",
    "        for k in im_ft_dict.keys():\n",
    "            im_ft_dict[k] = im_ft_dict[k].to(device)\n",
    "\n",
    "        im_label=im_label.to(device)\n",
    "        ft=torch.cat((im_ft_dict['tf'],im_ft_dict['gene']),dim=1)\n",
    "        ft=ft.squeeze(2)\n",
    "\n",
    "        logits,gnn_re,cell_embd=model(im_ft_dict,adj_dict)\n",
    "\n",
    "        loss=model.loss(gnn_re,ft,logits,im_label,args,cell_embd)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss = total_loss + loss.item()\n",
    "        global_step += args.batch_size \n",
    "\n",
    "\n",
    "        all_labels.extend(im_label.detach().cpu().numpy())\n",
    "        all_predictions.extend(F.softmax(logits,-1).detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    epoch_loss_train=total_loss/len(dataloader['train'])\n",
    "    train_labels = np.array(all_labels)    \n",
    "    train_predictions = np.argmax(np.array(all_predictions),axis=1)\n",
    "\n",
    "    model.eval()  \n",
    "    with torch.no_grad():    \n",
    "        total_loss_val = 0\n",
    "        val_labels = []\n",
    "        val_predictions = []    \n",
    "        for i, batch in enumerate(dataloader['valid']): \n",
    "            ft_dict,label=batch\n",
    "            for k in ft_dict.keys():\n",
    "                ft_dict[k] = ft_dict[k].to(device)\n",
    "            label=label.to(device)\n",
    "            ft=torch.cat((ft_dict['tf'],ft_dict['gene']),dim=1)\n",
    "            ft=ft.squeeze(2)\n",
    "\n",
    "            logits,gnn_re,cell_embd=model(ft_dict,adj_dict)\n",
    "            \n",
    "            loss=model.loss(gnn_re,ft,logits,label,args,cell_embd)      \n",
    "\n",
    "            total_loss_val = total_loss_val + loss.item()\n",
    "\n",
    "            val_labels.extend(label.cpu().numpy())\n",
    "            val_predictions.extend(F.softmax(logits,-1).detach().cpu().numpy())\n",
    "    \n",
    "    epoch_loss_val=total_loss_val/len(dataloader['valid'])\n",
    "    val_labels = np.array(val_labels)    \n",
    "    val_predictions = np.argmax(np.array(val_predictions),axis=1)\n",
    "\n",
    "    accuracy_val = accuracy_score(val_labels, val_predictions)\n",
    "    f1_val = f1_score(val_labels, val_predictions,average='macro')\n",
    "\n",
    "    if f1_val>best_val_f1:\n",
    "        best_val_f1=accuracy_val\n",
    "        best_model_weights=model.state_dict()  # save model weight\n",
    "\n",
    "    return epoch_loss_train,epoch_loss_val,accuracy_val,f1_val,cur_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 18.275768079255755 valid loss: 15.551719284057617 acc: 0.8754208754208754 f1_macro: 0.6393104968723973 learning rate: 0.01\n",
      "train loss: 4.064474582672119 valid loss: 4.580415058135986 acc: 0.9730639730639731 f1_macro: 0.9687376584270809 learning rate: 0.007737809374999998\n",
      "train loss: 3.9953305972249886 valid loss: 4.6282837867736815 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.005987369392383787\n",
      "train loss: 3.970906295274433 valid loss: 4.402698040008545 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.00463291230159753\n",
      "train loss: 3.948802057065462 valid loss: 4.3958250999450685 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0035848592240854188\n",
      "train loss: 3.9343292838648747 valid loss: 4.405804443359375 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.002773895731218338\n",
      "train loss: 3.9181151892009534 valid loss: 4.351572799682617 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0021463876394293728\n",
      "train loss: 3.9029330579858077 valid loss: 4.383655834197998 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0016608338398760717\n",
      "train loss: 3.8800033142692163 valid loss: 4.31126127243042 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0012208654873684796\n",
      "train loss: 3.8689728536103902 valid loss: 4.306458377838135 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0009446824413773763\n",
      "train loss: 3.861907871145951 valid loss: 4.304102039337158 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0007309772651287749\n",
      "train loss: 3.8406054095218054 valid loss: 4.263685035705566 acc: 0.9764309764309764 f1_macro: 0.9751958739958154 learning rate: 0.0005656162735025293\n"
     ]
    }
   ],
   "source": [
    "## train\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "val_acc_list=[]\n",
    "val_f1_list=[]\n",
    "for epoch in range(args.epochs):\n",
    "    train_loss,val_loss,val_acc,val_f1,cur_lr=train()\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_f1_list.append(val_f1)\n",
    "    if epoch%5 == 0:\n",
    "        print('train loss:',train_loss,'valid loss:',val_loss,'acc:',val_acc,'f1_macro:',val_f1,'learning rate:',cur_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25076/173943127.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model_weights=torch.load('/home/lyy/oie-HGCN/results/Muraro/HGCN_pure_20251204_1917.pth')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "dataset['test']=GCNDataset(args,ft_dict_test,label_test)\n",
    "dataloader['test']=DataLoader(dataset['test'],batch_size=args.batch_size,shuffle=False,pin_memory=True)\n",
    "\n",
    "model.load_state_dict(best_model_weights)\n",
    "model.eval() \n",
    "all_cell_embd=[] \n",
    "all_tf_embd=[]\n",
    "all_tg_embd=[]\n",
    "all_labels=[]\n",
    "all_predictions=[]\n",
    "for k in adj_dict.keys():\n",
    "        adj_dict[k]=adj_dict[k].to(device)\n",
    "with torch.no_grad():        \n",
    "    for i, batch in enumerate(dataloader['test']):\n",
    "        ft_dict,label=batch  \n",
    "        for k in ft_dict.keys():\n",
    "            ft_dict[k] = ft_dict[k].to(device)\n",
    "        for k in adj_dict.keys():\n",
    "            adj_dict[k] = adj_dict[k].to(device)\n",
    "        label=label.to(device)\n",
    "\n",
    "        logits,output_re,cell_embd=model(ft_dict,adj_dict)\n",
    "\n",
    "        all_cell_embd.extend(cell_embd.cpu().numpy())   \n",
    "        all_labels.extend(label.squeeze().detach().cpu().numpy())\n",
    "        all_predictions.extend(F.softmax(logits,-1).detach().cpu().numpy())\n",
    "\n",
    "test_labels=np.array(all_labels)\n",
    "test_pred = np.argmax(np.array(all_predictions),axis=1)\n",
    "test_predictions=np.array(all_predictions)\n",
    "test_embd=np.array(all_cell_embd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(y_true,y_pred,y_score):\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = precision_score(y_true, y_pred,average='weighted')\n",
    "    recall = recall_score(y_true, y_pred,average='weighted')\n",
    "    # 计算AUPRC\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    print(\"f1:{:.4f}, precision:{:.4f},recall:{:.4f}, AUPRC:{:.4f}\".format(f1,precision,recall,auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.9718, precision:0.9726,recall:0.9717, AUPRC:0.9869\n"
     ]
    }
   ],
   "source": [
    "evaluate(test_labels,test_pred,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import os\n",
    "path='./results/{}/'.format(args.dataset)\n",
    "os.makedirs(path,exist_ok=True)\n",
    "string='ScanNet_{}'.format(datetime.datetime.now().strftime('%Y%m%d_%H%M'))\n",
    "save_dir=path+'{}'.format(string)\n",
    "print(save_dir)\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(save_dir,'{}.pth'.format(string)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
